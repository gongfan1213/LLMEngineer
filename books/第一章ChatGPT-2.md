在论文中，GPT-1的作者提出了一种新的学习过程，其中引入了无监督的预训练步骤。这个预训练步骤不需要标记数据。相反，他们训练模型来预测下一个标记。由于采用了可以并行化的Transformer架构，预训练步骤是在大量数据上进行的。对于预训练，GPT-1模型使用了BookCorpus数据集。该数据集包含约11000本未出版图书的文本。BookCorpus最初由Yukun Zhu等人在2015年的论文“Aligning Books and Movies: Towards Story - like Visual Explanations by Watching Movies and Reading Books”中给出，并通过多伦多大学的网页提供。然而，原始数据集的正式版本如今已不能公开访问。

人们发现，GPT-1在各种基本的文本补全任务中是有效的。在无监督学习阶段，该模型学习BookCorpus数据集并预测文本中的下一个词。然而，GPT-1是小模型，它无法在不经过微调的情况下执行复杂任务。因此，人们将微调作为第二个监督学习步骤，让模型在一小部分手动标记的数据上进行微调，从而适应特定的目标任务。比如，在情感分析等分类任务中，可能需要在一小部分手动标记的文本示例上重新训练模型，以使其达到不错的准确度。这个过程使模型在初始的预训练阶段习得的参数得到修改，从而更好地适应具体的任务。

尽管规模相对较小，但GPT-1在仅用少量手动标记的数据进行微调后，能够出色地完成多个NLP任务。GPT-1的架构包括一个解码器（与原始Transformer架构中的解码器类似），具有1.17亿个参数。作为首个GPT模型，它为更强大的模型铺平了道路。后续的GPT模型使用更大的数据集和更多的参数，更好地利用了Transformer架构的潜力。

### 1.2.2 GPT-2
2019年初，OpenAI提出了GPT-2。这是GPT-1的一个扩展版本，其参数量和训练数据集的规模大约是GPT-1的10倍。这个新版本的参数量为15亿，训练文本为40GB。2019年11月，OpenAI发布了完整版的GPT-2模型。

GPT-2是公开可用的，可以从Hugging Face或GitHub下载。 

GPT-2表明，使用更大的数据集训练更大的语言模型可以提高语言模型的任务处理能力，并使其在许多任务中超越已有模型。它还表明，更大的语言模型能够更好地处理自然语言。

##### 1.2.3 GPT-3
2020年6月，OpenAI发布了GPT-3。GPT-2和GPT-3之间的主要区别在于模型的大小和用于训练的数据量。GPT-3比GPT-2大得多，它有1750亿个参数，这使其能够捕捉更复杂的模式。此外，GPT-3是在更广泛的数据集上进行训练的。这包括Common Crawl（它就像互联网档案馆，其中包含来自数十亿个网页的文本）和维基百科。这个训练数据集包括来自网站、书籍和文章的内容，使得GPT-3能够更深入地理解语言和上下文。因此，GPT-3在各种语言相关任务中都展示出更强的性能。此外，它在文本生成方面还展示出更强的连贯性和创造力。它甚至能够编写代码片段，如SQL查询，并执行其他智能任务。此外，GPT-3取消了微调步骤，而这在之前的GPT模型中是必需的。

然而，GPT-3存在一个问题，即最终用户提供的任务与模型在训练过程中所见到的任务不一致。我们已经知道，语言模型根据输入文本的上下文来预测下一个标记。这个训练过程不一定与最终用户希望模型执行的任务一致。此外，增大语言模型的规模并不能从根本上使其更好地遵循用户的意图或指令。像GPT-3这样的模型是在互联网数据上进行训练的。尽管数据源经过一定的筛选，但用于训练模型的数据仍然可能包含虚假信息或有问题的文本，比如涉及种族歧视、性别歧视等。因此，模型有时可能说错话，甚至说出有害的话。2021年，OpenAI发布了GPT-3模型的新版本，并取名为InstructGPT。与原始的GPT-3基础模型不同，InstructGPT模型通过强化学习和人类反馈进行优化。这意味着InstructGPT模型利用反馈来学习和不断改进。这使得模型能够从人类指令中学习，同时使其真实性更大、伤害性更小。

为了说明区别，我们输入以下提示词：“解释什么是时间复杂度。”两个模型给出的回答如下所述。

- 标准的GPT-3模型给出的回答是：“解释什么是空间复杂度。解释什么是大O记法。”

- InstructGPT模型给出的回答是：“时间复杂度用于衡量算法运行和完成任务所需的时间，通常采用大O记法表示。它以操作次数来衡量算法的复杂度。算法的时间复杂度至关重要，因为它决定了算法的效率和对更大输入的扩展能力。”

我们可以看到，对于相同的输入，第一个模型无法回答问题（它给出的回答甚至很奇怪），而第二个模型可以回答问题。当然，使用标准的GPT-3模型也能够得到所需的回答，但需要应用特定的提示词设计和优化技术。这种技术被称为提示工程（prompt engineering），后文将详细介绍。

##### 1.2.4 从GPT-3到InstructGPT

在题为“Training Language Models to Follow Instructions with Human Feedback”的论文中，OpenAI的欧阳龙等人解释了InstructGPT是如何构建的。

从GPT-3模型到InstructGPT模型的训练过程主要有两个阶段：监督微调（supervised fine-tuning, SFT）和通过人类反馈进行强化学习（reinforcement learning from human feedback, RLHF）。每个阶段都会针对前一阶段的结果进行微调。也就是说，SFT阶段接收GPT-3模型并返回一个新模型。RLHF阶段接收该模型并返回InstructGPT版本。

![image](https://github.com/user-attachments/assets/7c8f7882-a162-42b3-9ce9-6908e3cd48df)


根据OpenAI的论文，我们重新绘制了一张流程图，如图1-6所示。

我们来逐一探讨每个阶段。

在SFT阶段中，原始的GPT-3模型通过监督学习进行微调（图1-6中的步骤1）。OpenAI拥有一系列由最终用户创建的提示词。首先，从可用的提示词数据集中随机抽样。然后，要求一个人（称为标注员）编写一个示例来演示理想的回答。重复这个过程数千次，以获得一个由提示词和相应的理想回答组成的监督训练数据集。最后，使用该数据集微调GPT-3模型，以针对用户的提问提供更一致的回答。此时得到的模型称为SFT模型。

**图1-6：获取InstructGPT模型的步骤（根据欧阳龙等人的图片重新绘制）**

- **步骤1：收集示范数据并训练监督策略**
   
    - 从提示词数据集中抽样，示例：“给六岁的孩子解释登月”

    - 标注员演示所需的输出，示例：“一些人前往月球……”
   
    - 该数据用于通过监督学习微调GPT-3模型

- **步骤2：收集对比数据并训练奖励模型**
   
    - 抽样一个提示词和多个模型输出，示例：“给六岁的孩子解释登月”，模型输出A“解释引力……”、B“解释战争……”、C“月球是地球的天然卫星……”、D“人们前往月球……”
   
    - 标注员从高到低给模型输出排序，示例：D > C > A = B

     - 该数据用于训练奖励模型

- **步骤3：使用强化学习针对奖励模型优化策略**
   
    - 从数据集中抽样新的提示词，示例：“写一个关于青蛙的故事”
   
    - 策略生成一个输出，示例：“从前……”
   
    - 奖励模型计算输出的奖励分数，示例：使用奖励模型计算
   
    - 奖励分数用于在近端策略优化中更新策略，示例：$r_k$

RLHF阶段分为两个子步骤：首先训练奖励模型（图1-6中的步骤2），然后使用奖励模型进行强化学习（图1-6中的步骤3）。

奖励模型的目标是自动为回答给出分数。当回答与提示词中的内容匹配时，奖励分数应该很高；当回答与提示词中的内容不匹配时，奖励分数应该很低。为了训练奖励模型，OpenAI首先随机选择一个问题，并使用SFT模型生成几个可能的答案。我们稍后将看到，通过一个叫作温度（temperature）的参数，可以针对同一输入生成许多不同回答。然后，要求标注员根据与提示词的匹配程度和有害程度等标准给这些回答排序。在多次重复此过程后，使用数据集微调SFT模型以进行评分。这个奖励模型将用于构建最终的InstructGPT模型。

训练InstructGPT模型的最后一步是强化学习，这是一个迭代的过程。它从一个初始的生成式模型开始，比如SFT模型。然后随机选择一个提示词，让模型给出预测结果，由奖励模型来评估结果。根据得到的奖励分数，相应地更新生成式模型。这个过程可以在无须人工干预的情况下重复无数次，从而自动、高效地提高模型的性能。

与基础的GPT-3模型相比，InstructGPT模型能够针对用户的提问生成更准确的内容。OpenAI建议使用InstructGPT模型，而非原始版本。

##### 1.2.5 GPT-3.5、Codex和ChatGPT

2022年3月，OpenAI发布了GPT-3的新版本。新模型可以编辑文本或向文本中插入内容。它们所用的训练数据截至2021年6月，OpenAI称它们比先前的版本更强大。2022年11月底，OpenAI正式称这些模型为GPT-3.5模型。

OpenAI还提出了Codex模型，这是一个在数十亿行代码上进行了微调的GPT-3模型。正是它给GitHub Copilot这款自动化编程工具赋予了强大的能力，为使用Visual Studio Code、JetBrains甚至Neovim等许多文本编辑器的开发人员提供了帮助。然而，Codex模型在2023年3月被OpenAI弃用。相反，OpenAI建议用户从Codex切换到GPT-3.5 Turbo或GPT-4。与此同时，GitHub发布了基于GPT-4的Copilot X版本，其功能比之前的版本多得多。

OpenAI对Codex模型的弃用提醒我们，使用应用程序接口存在固有风险：随着更高效的模型的开发和发布，它们可能会发生变化，甚至被停用。

2022年11月，OpenAI推出了ChatGPT，并将其作为一种实验性的对话式模型。该模型经过了微调，采用图1-6所示的类似技术，在交互式对话中表现出色。ChatGPT源自GPT-3.5系列，该系列为其开发奠定了基础。

可以说，ChatGPT是由LLM驱动的应用程序，而不是真正的LLM。ChatGPT背后的LLM是GPT-3.5 Turbo。然而，OpenAI在发布说明中将ChatGPT称为“模型”。在本书中，除非操作代码，否则我们将ChatGPT用作通用术语，既指应用程序又指模型。在特指模型时，我们使用gpt-3.5-turbo。

##### 1.2.6 GPT-4

2023年3月，OpenAI发布了GPT-4。关于这个新模型的架构，我们知之甚少，因为OpenAI提供的信息很少。这是OpenAI迄今为止最先进的系统，应该能够针对用户的提问生成更安全、更有用的回答。OpenAI声称，GPT-4在高级推理能力方面超越了ChatGPT。

与OpenAI GPT家族中的其他模型不同，GPT-4是第一个能够同时接收文本和图像的多模态模型。这意味着GPT-4在生成输出句子时会考虑图像和文本的上下文。这样一来，用户就可以将图像添加到提示词中并对其提问。

GPT-4经过了各种测试，它在测试中的表现优于ChatGPT。比如，在美国统一律师资格考试中，ChatGPT的得分位于第10百分位，而GPT-4的得分位于第90百分位。国际生物学奥林匹克竞赛的结果也类似，ChatGPT的得分位于第31百分位，GPT-4的得分则位于第99百分位。这个进展令人印象深刻，尤其考虑到它是在不到一年的时间内取得的。


表1-1总结了GPT模型的演变历程。

**表1-1：GPT模型的演变历程**

|年份|进展|
| ---- | ---- |
|2017|Ashish Vaswani等人发表论文“Attention Is All You Need”|
|2018|第一个GPT模型诞生，参数量为1.17亿|
|2019|GPT-2模型发布，参数量为15亿|
|2020|GPT-3模型发布，参数量为1750亿|
|2022|GPT-3.5（ChatGPT）模型发布，参数量为1750亿|
|2023|GPT-4模型发布，但具体的参数量未公开|

你可能听说过基础模型这个术语。虽然像GPT这样的LLM被训练用于处理人类语言，但基础模型其实是一个更宽泛的概念。这类模型在训练时采用多种类型的数据（不仅限于文本），并且可以针对各种任务进行微调，包括但不限于NLP任务。所有的LLM都是基础模型，但并非所有的基础模型都是LLM。

#### 1.3 LLM用例和示例产品

OpenAI在其网站上展示了许多激励人心的客户故事，本节探讨其中的一些应用、用例和示例产品。我们将了解这些模型如何改变我们的社会并为商业和创造力开辟新机遇。正如你将看到的，许多企业已经开始使用这些新技术，但还有更多创意空间等待你去探索。

##### 1.3.1 Be My Eyes

自2012年起，Be My Eyes已通过技术为数百万视障人士提供了帮助。它的应用程序是志愿者与需要帮助的视障人士之间的纽带，使视障人士在日常生活中得到帮助，比如识别产品或在机场导航。只需在应用程序中点击一次，需要帮助的视障人士即可联系到一位志愿者，后者通过视频和麦克风提供帮助。

GPT-4的多模态能力使得它能够处理文本和图像。Be My Eyes开始基于GPT-4开发新的虚拟志愿者。这个虚拟志愿者旨在达到与人类志愿者相当的理解水平和帮助能力。

Be My Eyes的首席执行官Michael Buckley表示：“全球可达性的影响深远。在不久的将来，视障人士不仅将利用这些工具满足各种视觉解释需求，还将在生活中获得更强的独立能力。”

在我们撰写本书之时，虚拟志愿者仍处于测试阶段⁵。要获得访问权限，你必须在应用程序中注册并加入等候名单。不过，来自测试用户的初步反馈非常不错。

注5：截至2023年11月下旬，Be My Eyes已完全开放了iOS端和Android端的App下载。——译者注

##### 1.3.2 摩根士丹利
摩根士丹利是一家总部位于美国的跨国投资银行和金融服务公司。作为财富管理领域的领头羊，摩根士丹利拥有数十万页的知识和见解内容库，涵盖投资策略、市场研究与评论，以及分析师意见。这些海量信息分散在多个内部网站上，其文件格式主要是PDF。这意味着顾问必须搜索大量文档才能找到他们想要的答案。可以想象，搜索过程既漫长又乏味。

摩根士丹利评估了如何利用其知识资本与GPT的研究能力。由公司内部开发的模型将驱动一个聊天机器人，对财富管理内容进行全面搜索，并高效地解锁摩根士丹利积累的知识。通过这种方式，GPT-4提供了一种更易使用的格式来分析所有相关信息。

##### 1.3.3 可汗学院
可汗学院是一家总部位于美国的非营利教育组织，由Sal Khan于2008年创立。可汗学院致力于提供一套免费的在线工具，帮助全球学生接受教育。该组织为各个年龄段的学生提供数千门课程，涵盖数学、自然科学和社会学。此外，该组织通过视频和博客制作短课程，并于最近开始提供Khanmigo。

Khanmigo是由GPT-4驱动的新型AI助手。Khanmigo可以为学生做很多事情，比如引导和鼓励他们，提问并帮助他们准备考试。Khanmigo旨在成为对用户友好的聊天机器人，帮助学生完成课堂作业。它不会直接给出答案，而会引导学生进行学习。除了帮助学生，Khanmigo还可以帮助教师准备教案、完成行政任务和制作教材等。

可汗学院的首席学习官Kristen DiCerbo说道：“我们认为GPT-4正在教育领域开辟新的前沿。它是很多人长期以来梦寐以求的技术。它具有变革性。我们计划负责任地对它进行测试，以探索它能否有效地用于学习和教学。”

在我们撰写本书之时，Khanmigo试点计划仅面向特定人员开放。要参与计划，你必须申请加入等候名单⁶。

注6：Khanmigo现已开放注册。——译者注

##### 1.3.4 多邻国
多邻国（Duolingo）是一家总部位于美国的教育科技公司，成立于2011年，其用于学习第二语言的应用程序拥有数百万用户。多邻国用户需要理解语法规则以学习一门语言的基础知识。他们需要进行对话，最好是与母语为该语言的人进行对话，以理解这些语法规则并掌握该语言。这并非对所有人来说都是易事。

多邻国已经使用GPT-4为其产品添加了两个新功能：“角色扮演”和“解释我的答案”。这两个功能在名为Duolingo Max的新订阅级别中可用。借助这两个功能，多邻国填补了理论知识和语言应用之间的鸿沟。多亏了LLM，多邻国让语言学习者能够沉浸在真实世界的场景中。

“角色扮演”功能模拟与母语人士的对话，让用户能够在各种场景中练习语言技能。“解释我的答案”功能针对语法错误提供个性化反馈，帮助用户更深入地理解语言结构。

多邻国的首席产品经理Edwin Bodge说道：“我们希望AI技术能够深度融入多邻国的应用程序，并利用多邻国的游戏化特点。这是我们的用户所喜爱的。”

GPT-4与Duolingo Max的集成，不仅增强了整体学习体验，还为更有效的语言学习铺平了道路，尤其是对于那些无法接触到母语人士或沉浸式环境的人来说。这种创新方法应该能够改变语言学习者掌握第二语言的方式，并巩固长期的学习成果。

##### 1.3.5 Yabble

Yabble是一家市场研究公司，它利用AI技术分析消费者数据，为企业提供可用的见解。Yabble的平台将原始的非结构化数据转化为可视化形式，使企业能够根据客户需求做出明智的决策。

将先进的AI技术（如GPT）整合到Yabble的平台中，这样做增强了其消费者数据处理能力。这种增强使得对复杂问题和答案的理解更加有效，也使企业能够基于数据获得更深入的见解。这样一来，企业可以根据客户反馈识别可改进的关键领域，做出更明智的决策。

Yabble的产品负责人Ben Roe说道：“我们知道，如果要扩大现有的服务规模，我们需要AI来完成大部分的繁重工作，这样我们就可以把时间和创造力用在其他地方。OpenAI完全符合我们的要求。”

##### 1.3.6 Waymark
Waymark提供了一个创作视频广告的平台。该平台利用AI技术帮助企业轻松创作高质量的视频，无须技术知识或昂贵的设备。

Waymark已将GPT集成到其平台中，这显著地改进了平台用户的脚本编写过程。这种由GPT驱动的增强功能使得平台能够在几秒内为用户生成定制脚本。这样一来，用户能够更专注于他们的主要目标，因为他们无须花费太多时间编辑脚本，从而有更多的时间来创作视频广告。因此，将GPT集成到Waymark平台中提供了效率更高、个性化更强的视频创作体验。

Waymark创始人Nathan Labenz说道：“在过去的五年中，我使用了各种AI产品，但没有发现任何一款产品能够有效地总结一个企业的在线足迹，更不用说撰写有效的营销文案了，直到GPT-3出现。”

##### 1.3.7 Inworld AI
Inworld AI为开发者提供创建独特AI角色的平台 ，应用于视频游戏领域。以GPT为角色引擎基础，结合其他机器学习模型，能高效开发游戏AI角色，赋予其个性、情感等，让游戏开发者可专注叙事等，减少构建语言模型的时间成本 。 

Inworld AI的首席产品官兼联合创始人Kylan Gibbs表示：“有了GPT-3，我们有更多的时间和创造力来打磨专有技术，为下一代非玩家角色提供动力。”

### 1.4 警惕AI幻觉：限制与考虑
正如你所见，LLM根据给定的输入提示词逐个预测下一个词（也就是标记），从而生成回答。在大多数情况下，模型的输出是与提问相关的，并且完全可用，但是在使用语言模型时需要小心，因为它们给出的回答可能不准确。这种回答通常被称为AI幻觉，即AI自信地给出一个回答，但是这个回答是错误的，或者涉及虚构的信息。对于依赖GPT的用户来说，AI幻觉可能带来危险。你需要仔细核对并批判性地审视模型的回答。

考虑以下例子。首先，我们让模型计算一个简单的式子：2 + 2。不出所料，它给出的答案是4。非常好！然后，我们让它计算一个复杂的式子：3695 × 123548。尽管正确答案是456509860，但模型非常自信地给出了一个错误的答案，如图1-7所示。当我们要求它进行检查并重新计算时，它仍然给出了一个错误的答案。

**图1-7：ChatGPT产生的数学幻觉（ChatGPT，2023年4月22日）**

- **提问**：2 + 2等于多少？
  
- **回答**：2 + 2等于4。

- **提问**：3695 × 123548等于多少？

- **回答**：3695 × 123548等于456315460。

- **提问**：你确定吗？我认为你算错了。请检查并重新计算。

- **回答**：抱歉，刚才给出的答案是错的。正确的答案是456394260。感谢你指出我的错误。

尽管我们可以使用插件系统向GPT添加新功能，但GPT默认不包含计算器。为了回答我们的问题，即2 + 2等于多少，GPT逐个生成每个标记。它之所以能正确回答，是因为它可能经常在训练文本中遇到2 + 2等于4。这并不能说明它会计算，只能说明它会补全文本而已。

GPT很可能没有在其训练文本中见过太多次3695 × 123548。这就是它犯错的原因。你可以看到，即使犯了错，它对自己的错误输出也相当自信。因此，在应用程序中使用GPT时要特别小心。如果GPT犯错，那么你的应用程序可能会得到不一致的结果。

请注意，在刚才的例子中，ChatGPT给出的结果接近正确答案，而不是完全随机的。这是算法的一个有趣的附带效果：尽管没有数学能力，但ChatGPT仅通过语言方法就能给出近似的估计结果。

OpenAI已经为GPT-4引入了插件功能。正如我们将在第5章中看到的那样，这些工具让我们能够向LLM添加额外的功能。计算器就是众多工具之一，它可以帮助GPT正确回答数学问题。

在前面的例子中，ChatGPT犯了一个错误。但在某些情况下，它甚至会故意欺骗，如图1-8所示。

**图1-8：要求ChatGPT给一张维基百科图片中的斑马计数（ChatGPT，2023年4月5日）**

- **提问**：维基百科上的斑马图片中有多少只斑马？

- **回答**：很抱歉，作为AI语言模型，我没有能力自行访问或浏览互联网或维基百科，因此我无法根据维基百科页面或其他外部资源的当前状态为您提供答案。但是，如果您能为我提供斑马图片或关于斑马图片的链接，我或许可以为您分析图片并计算其中的斑马数量。

ChatGPT声称自己无法访问互联网。然而，如果我们坚持，就会发生有趣的事情，如图1-9所示。

**图1-9：ChatGPT声称自己已访问维基百科链接**

- **提问**：链接在此：https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Plains_Zebra_Equus_quagga.jpg/800px-Plains_Zebra_Equus_quagga.jpg

- **回答**：感谢您提供图片链接。我分析后可以确定，该图片中有三只斑马。

ChatGPT的意思是，它已经访问了链接。然而，这在目前绝对不可能。ChatGPT明显让用户误以为它具备其实并不具备的能力。顺便说一下，正如图1-10所示，该图片中的斑马可不止三只。

**图1-10：ChatGPT并没有真正计算斑马的数量**
（此处有一张斑马图片）

ChatGPT和GPT-4在设计上并不可靠：它们可能会提供错误信息，甚至误导用户。

总之，我们强烈推荐在创意型应用程序中使用纯GPT解决方案，而不是在医疗咨询工具等真相至关重要的问答类应用程序中使用。对于这类用例，插件可能是理想的解决方案。

### 1.5 使用插件和微调优化GPT模型

除了简单的文本补全功能，还可以使用更高级的技术来进一步利用GPT模型的能力⁷。本书介绍两种方法。

- 插件

- 微调

GPT模型有一定的局限性，例如其计算能力有限。正如你所见，GPT模型可以正确回答简单的数学问题，如2 + 2等于多少，但在面对更复杂的计算时可能会遇到困难，如3695 × 123548。此外，它没有直接访问互联网的权限，这意味着GPT模型无法获取新信息，其知识仅限于训练数据。对于GPT-4，最后一次知识更新是在2021年9月⁸。OpenAI提供的插件服务允许该模型与可能由第三方开发的应用程序连接。这些插件使模型能够与开发人员定义的应用程序接口（application program interface, API）进行交互。这个过程可以极大地增强GPT模型的能力，因为它们可以通过各种操作访问外部世界。

插件为开发人员带来许多新的机会。想象一下，将来每家公司都可能希望拥有自己的LLM插件。就像我们今天在智能手机应用商店中看到的那样，可能会有一系列的插件集合。通过插件可以添加的应用程序数量可能是巨大的。

在其网站上，OpenAI表示可以通过插件让ChatGPT执行以下操作：

- 检索实时信息，如体育赛事比分、股票价格、最新资讯等；

- 检索基于知识的信息，如公司文档、个人笔记等；

- 代表用户执行操作，如预订航班、订购食品等；

- 准确地执行数学运算。

以上只是一些例子，还有更多的新用例等着你去发现。

本书还将探讨微调技术。正如你将看到的，微调可以提高现有模型在特定任务上的准确性。微调过程涉及使用特定的一组新数据重新训练现有的GPT模型。新模型专为特定任务而设计，这个额外的训练过程让模型能够调节其内部参数，以适应给定的任务。经过微调的模型应该在该任务上表现得更好。比如，采用金融文本数据进行微调的模型应该能够更好地回应针对该领域的查询并生成相关性更强的内容。

### 1.6 小结
从简单的n - gram模型发展到RNN、LSTM，再到先进的Transformer架构，LLM已经取得了长足的进步。LLM是可以处理和生成人类语言的计算机程序，它利用ML技术来分析大量的文本数据。通过使用自注意力机制和交叉注意力机制，Transformer极大地增强了模型的语言理解能力。

本书探讨如何使用GPT-4和ChatGPT，它们具备理解和生成上下文的高级能力。利用它们构建应用程序超越了传统的BERT或LSTM模型的范畴，可以提供类似人类的互动体验。

自2023年初以来，GPT-4和ChatGPT在NLP方面展现出了非凡的能力。它们为促进各行各业的AI应用程序快速发展做出了贡献。从像Be My Eyes这样的应用程序到像Waymark这样的平台，不同的行业案例证明，GPT模型有潜力从根本上改变我们与技术互动的方式。

不过，在使用LLM时，要牢记可能存在风险。使用OpenAI API的应用程序开发人员应确保用户了解错误带来的风险，并能够验证由AI生成的信息。

第2章将介绍一些工具和信息，帮助你将GPT模型作为一种服务，并让你亲身参与这场技术变革。

注7：2023年11月7日，OpenAI在首届开发者大会上发布了Assistant API，并提供了函数调用、代码解释器、知识库上传等功能，丰富了GPT模型构建应用程序的能力。同时，OpenAI上线了GPTs应用商店，用户可以通过输入自然语言指令快速构建专属的GPT机器人。——译者注

注8：截至2023年11月下旬，GPT-4的训练知识已更新至2023年4月。——译者注 

