#### 3.4.3 项目3：打造《塞尔达传说：旷野之息》专家

本项目的目标是让ChatGPT回答它在训练阶段没有见过的问题，因为涉及的数据要么是私密的，要么在2021年之前不可用。

对于这个例子，我们使用任天堂提供的关于视频游戏《塞尔达传说：旷野之息》的指南。因为ChatGPT已经对《塞尔达传说：旷野之息》比较了解了，所以本例仅供演示目的。你可以将此PDF文件替换为其他数据。

本项目的目标是构建一个AI助手，它能根据任天堂指南的内容回答关于《塞尔达传说：旷野之息》的问题。

由于这个PDF文件太大了，无法通过提示词发送给OpenAI模型，因此我们必须使用其他解决方案。有两种方法可用于将ChatGPT的功能与你自己的数据集集成：

- **微调**：针对特定的数据集重新训练现有模型。

- **少样本学习**：向提示词中添加示例。


第4章将详细介绍这两种方法。在这里，我们使用另一种以软件为导向的方法。基本思想是使用GPT - 4或ChatGPT进行信息还原，而不是信息检索：我们不指望AI模型知道问题的答案。相反，我们要求它根据我们认为可能与问题匹配的文本摘录来生成答案。这就是我们在本项目中要做的事情。

![image](https://github.com/user-attachments/assets/8ea9eb90-3337-4b1d-a032-9e527141fc0d)



图3 - 5展示了基本原理。

图3 - 5：类ChatGPT解决方案的原理，其中使用了自己的数据

你需要以下三个组件：
- **意图服务**：

当用户向应用程序提问时，意图服务的作用是检测用户的意图。用户所提的问题与你的数据相关吗？也许你有多个数据源，意图服务应该检测出需要使用的数据源。该服务还可以检测用户所提的问题是否遵守OpenAI的使用规则，或者是否包含敏感信息。本项目中的意图服务将基于一个OpenAI模型。

- **信息检索服务**：


该服务将获取意图服务的输出并检索正确的信息。这意味着你已经准备好数据，并且数据在该服务中可用。在本项目中，我们将比较自己的数据和用户查询之间的嵌入。嵌入将使用OpenAI API生成并存储在向量存储系统中。 

- **响应服务**：


该服务将使用信息检索服务的输出，并从中生成用户所提问题的答案。我们再次使用OpenAI模型生成答案。

请从随书文件包中查看本示例的完整代码。在接下来的内容中，你只会看到最重要的代码片段。

1. **Redis**

Redis是一个开源数据结构存储系统，通常用作基于内存的键 - 值数据库或消息代理。本项目使用Redis的两个内置功能：向量存储能力和向量相似性搜索解决方案。若想了解更多信息，请查阅Redis文档。

我们先使用Docker启动一个Redis实例。作为示例，随书文件包提供了基本的redis.conf文件和docker - compose.yml文件。

2. **信息检索服务**

首先，我们初始化一个Redis客户端：
```python
class DataService():
    def __init__(self):
        # 连接Redis
        self.redis_client = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            password=REDIS_PASSWORD
        )
```
接着，我们初始化一个函数，从PDF文件中创建嵌入。通过`from pypdf import PdfReader`导入PdfReader库并读取PDF文件。

以下函数读取PDF中的所有页面，将其分割为预定义长度的块，然后调用OpenAI嵌入端点，如第2章所示。

```python
def pdf_to_embeddings(self, pdf_path: str, chunk_length: int = 1000):
    # 从PDF文件中读取数据并将其拆分为块
    reader = PdfReader(pdf_path)
    chunks = []
    for page in reader.pages:
        text_page = page.extract_text()
        chunks.extend([text_page[i:i+chunk_length]
                       for i in range(0, len(text_page), chunk_length)])
    # 创建嵌入
    response = openai.Embedding.create(model='text-embedding-ada-002',
                                       input=chunks)
    return [{'id': value['index'],
             'vector':value['embedding'],
              'text':chunks[value['index']]} for value]
```

第5章将介绍另一种方法：使用插件或LangChain框架阅读PDF文件。

此函数返回一个具有属性id、vector和text的对象列表。id属性是块的编号，text属性是原始文本块本身，vector属性是由OpenAI服务生成的嵌入。vector属性将在之后用于搜索。为此，我们创建`load_data_to_redis`函数来执行实际的数据加载工作。

```python
def load_data_to_redis(self, embeddings):
    for embedding in embeddings:
        key = f"{PREFIX}:{str(embedding['id'])}"
        embedding["vector"] = np.array(embedding["vector"], dtype=np.float32).tobytes()
        self.redis_client.hset(key, mapping=embedding)
```

以上只是代码片段。在将数据加载到Redis之前，需要初始化Redis索引和RedisSearch字段。若要了解详细信息，请查看随书文件包中的代码。

数据服务现在需要一个方法来根据用户输入创建一个嵌入向量，并使用它查询Redis：

```python
def search_redis(self,user_query: str):
    # 根据用户输入创建嵌入向量
    embedded_query = openai.Embedding.create(
        input=user_query,
        model="text-embedding-ada-002")["data"][0]['embedding']
    # 使用Redis语法准备好查询（请参阅随书文件包中的完整代码），然后执行向量搜索
    results = self.redis_client.ft(index_name).search(query, params_dict)
    return [doc['text'] for doc in results.docs]
```

向量搜索返回我们在上一步中插入的文档。由于后续步骤不需要向量格式，因此这里返回一个文本结果列表。

总结一下，DataService的结构如下所示。

```
DataService
    __init__
    pdf_to_embeddings
    load_data_to_redis
    search_redis
```

通过以更智能的方式存储数据，可以大幅提高应用程序的性能。在本例中，我们根据固定数量的字符进行了基本的分块操作。你可以根据段落或句子进行分块，或者找到一种将段落标题与其内容关联起来的方法。

3. **意图服务**

在面向真实用户的应用程序中，你可以将所有过滤用户问题的逻辑放入意图服务的代码中。比如，你可以检测问题是否与你的数据集相关（如果不相关，则返回一条通用的拒绝消息），或者添加机制来检测恶意意图。然而，本例中的意图服务非常简单——它使用ChatGPT模型从用户所提的问题中提取关键词。

```python
class IntentService():
    def __init__(self):
        pass
    def get_intent(self, user_question: str):
        # 调用ChatCompletion端点
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user",
                 "content": f"""Extract the keywords from the following question: {user_question}."""}
            ]
        )
        # 提取响应
        return (response['choices'][0]['message']['content'])
```

在意图服务示例中，我们使用了一个基本的提示词，让模型从问题中提取关键词，并且只回答关键词，不要回答其他任何内容。我们鼓励你尝试多个提示词，看看哪个最有效，并添加对滥用应用程序的检测机制。

4. **响应服务**

响应服务很简单。我们使用提示词来要求ChatGPT模型根据数据服务找到的文本来回答问题：

```python
class ResponseService():
    def __init__(self):
        pass
    def generate_response(self, facts, user_question):
        # 调用ChatCompletion端点
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user",
                 "content": f"""Based on the FACTS, answer the QUESTION. QUESTION: {user_question}. FACTS: {facts}"""}
            ]
        )
        # 提取响应
        return (response['choices'][0]['message']['content'])
```

这里的关键是提示词`Based on the FACTS, answer the QUESTION. QUESTION: {user_question}. FACTS: {facts}`，它精确地指示模型给出良好的结果。

5. **整合所有内容**

初始化数据：

```python
def run(question: str, file: str='ExplorersGuide.pdf'):
    data_service = DataService()
    data = data_service.pdf_to_embeddings(file)
    data_service.load_data_to_redis(data)
```

获取意图：

```python
intent_service = IntentService()
intents = intent_service.get_intent(question)
```

获取事实：

```python
facts = service.search_redis(intents)
```

获得答案：

```python
return response_service.generate_response(facts, question)
```

为了看看效果，我们提一个问题：在哪里可以找到宝箱（Where to find treasure chests）? 模型给出了以下答案。

```
You can find treasure chests scattered around Hyrule, in enemy bases, underwater, in secret corners of shrines, and even hidden in unusual places. Look out for towers and climb to their tops to activate them as travel gates and acquire regional map information. Use your Magnesis Rune to fish out chests in water and move platforms. Keep an eye out for lively Koroks who reward you with treasure chests.
```

在第5章中，你可以找到使用LangChain框架或插件构建类似项目的其他方法。

在本项目中，我们最终得到了一个ChatGPT模型，它似乎已经学习了我们自己的数据，而实际上我们并没有将完整的数据发送给OpenAI或重新训练模型。你可以进一步以更智能的方式构建嵌入，以更好地适应你的文档，比如将文本分成段落而不是固定长度的块，或者将段落标题作为Redis向量数据库对象的属性之一。从使用LLM这个方面来看，本项目无疑令人印象深刻。不过请记住，第5章介绍的LangChain框架可能更适合大型项目。
