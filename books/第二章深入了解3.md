用户消息相当于是用户在ChatGPT网页界面中键入的问题或句子。它既可以由应用程序的用户生成，也可以作为指令设置。

表2-1：必需的输入参数¹¹

| 字段名称 | 类型 | 描述 |
| ---- | ---- | ---- |
| model | 字符串 | 所选模型的ID。目前可用的有gpt-4、gpt-4-0613、gpt-4-32k、gpt-4-32k-0613、gpt-3.5-turbo、gpt-3.5-turbo-0301、gpt-3.5-turbo-0613、gpt-3.5-turbo-1106、gpt-3.5-turbo-16k、gpt-3.5-turbo-16k-0613。可以使用openai.Model.list()。请注意，并不是所有可用的模型都与openai.ChatCompletion兼容 |
| messages | 数组 | 表示对话的消息对象数组。消息对象有两个属性：role（可能的值有system、user和assistant）和content（包含对话消息的字符串） |

助手消息有两个作用：要么存储先前的回复以继续对话，要么设置为指令，以提供所需行为的示例。由于模型没有任何关于历史请求的“记忆”，因此存储先前的消息对于给出对话上下文和提供所有相关信息是必要的。

### 2. 对话长度和标记数量

如前所述，对话的总长度与标记的总数相关。这将影响以下方面。

- **成本**：定价基于标记计算。

- **时间**：标记越多，响应所需的时间就越长——最长可能需要几分钟。 

- **模型是否工作**：标记总数必须小于模型的上限。你可以在2.7节中找到关于标记数限制的示例。


正如你所见，有必要小心控制对话的长度。你可以通过管理消息的长度来控制输入标记的数量，并通过max_tokens参数来控制输出标记的数量。

OpenAI提供了一个名为tiktoken的库，让开发人员能够计算文本字符串中的标记数。我们强烈建议在调用端点之前使用此库来估算成本。

### 3. 可选参数

OpenAI提供了其他几个选项来微调用户与库的交互方式。这里不会详细说明所有参数，但我们建议你仔细查看表2-2。

表2-2：一些可选参数¹²

| 字段名称 | 类型 | 描述 |
| ---- | ---- | ---- |
| functions | 数组 | 由可用函数组成的数组，详见2.5.3节 |
| function_call | 字符串或对象 | 控制模型的响应方式：<br> - none表示模型必须以标准方式响应用户<br> - {"name": "my_function"}表示模型必须给出使用指定函数的回答<br> - auto表示模型可以在以标准方式响应用户和functions数组定义的函数之间进行选择 |
| temperature | 数值（默认值为1；可接受介于0和2之间的值） | 温度为0意味着对于给定的输入，对模型的调用很可能会返回相同的结果。尽管响应结果会高度一致，但OpenAI不保证确定性输出。温度值越高，结果的随机性就越强。LLM通过预测一系列标记来生成回答。根据输入上下文，LLM为每个潜在的标记分配概率。当温度被设置为0时，LLM将始终选择概率最高的标记。较高的温度为产生更多样化、更具创造性的输出 |
| n | 整型（默认值为1） | 通过设置这个参数，可以为给定的输入消息生成多个回答。不过，如果将温度设为0，那么虽然可以得到多个回答，但它们将完全相同或非常相似 |
| stream | 布尔型（默认值为false） | 顾名思义，这个参数将允许回答以流的格式呈现。这意味着当回答的内容较长时，它可以提供更好的用户体验 |
| max_tokens | 整型 | 这个参数指定在聊天中生成的最大标记数。虽然它是可选参数，但我们强烈建议将其设置为合适的值，以控制成本。请注意，如果该参数设置得过大，那么可能会被OpenAI忽略：输入和生成的标记总数不能超过模型的上限 |

你可以在OpenAI的文档中找到更多详细信息。

### 2.5.2 ChatCompletion端点的输出格式

你已经知道如何使用基于聊天模式的模型，让我们看看如何使用模型输出的结果。

以下是Hello World示例程序的完整响应：

```json
{
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "message": {
                "content": "Hello there! How may I assist you today?",
                "role": "assistant"
            }
        }
    ],
    "created": 1681134595,
    "id": "chatcmpl-73mC3tb0lMMHGcig3ygyy9nAxIP2vsU",
    "model": "gpt-3.5-turbo",
    "object": "chat.completion",
    "usage": {
        "completion_tokens": 10,
        "prompt_tokens": 11,
        "total_tokens": 21
    }
}
```

表2-3列出了生成的输出。¹³

| 字段名称 | 类型 | 描述 |
| ---- | ---- | ---- |
| choices | 对象数组 | 包含模型实际响应的数组。默认情况下，该数组只有一个元素，可以通过参数n（见表2-2）进行更改。该元素包含以下内容：<br> - finish_reason（字符串）：回答结束的原因。在Hello World示例程序中，finish_reason是stop，这意味着我们从模型中得到了完整的响应。如果在输出生成过程中出现错误，那么将体现在该字段中<br> - index（整型）：从choices数组中选择对象的索引<br> - message（对象）：包含一个role和一个content或function_call。role的值始终是assistant，content包括模型生成的文本。我们通常希望获得这样的字符串：response['choices'][0]['message']['content']。有关如何使用function_call，请参见2.5.3节 |
| created | 时间戳 | 生成时的时间戳。在Hello World示例程序中，这个时间戳转换为2023年4月10日星期一 下午1:49:55 |
| id | 字符串 | OpenAI内部使用的技术标识符 |
| model | 字符串 | 所用的模型。这与作为输入设置的模型相同 |
| object | 字符串 | 对于GPT-4模型和GPT-3.5模型，这始终应为chat.completion，因为我们使用的是ChatCompletion端点 |
| usage | 字符串 | 提供有关在此查询中使用的标记数的信息，从而为你提供费用信息。prompt_tokens表示输入中的标记数，completion_tokens表示输出中的标记数。你可能已经猜到了，total_tokens = prompt_tokens + completion_tokens |

如果将参数n设置为大于1，那么你会发现prompt_tokens的值不会改变，但completion_tokens的值将大致变为原来的n倍。

### 2.5.3 从文本补全到函数
OpenAI使其模型可以输出一个包含函数调用参数的JSON对象。模型本身无法调用该函数，但可以将文本输入转换为可由调用者以编程方式执行的输出格式。
在OpenAI API调用结果需要由代码的其余部分处理时，这个功能特别有用：你可以使用函数定义将自然语言转换为API调用或数据库查询，从文本中提取结构化数据，并通过调用外部工具来创建聊天机器人，而无须创建复杂的提示词以确保模型以特定的格式回答可以由代码解析的问题。
函数定义需要作为函数对象数组传递。表2-4列出了函数对象的详细信息¹⁴。
注11：截至2023年11月下旬，ChatCompletion的可用模型包括gpt-3.5-turbo、gpt-3.5-turbo-0301、gpt-3.5-turbo-0613、gpt-3.5-turbo-1106、gpt-3.5-turbo-16k、gpt-3.5-turbo-16k-0613。——译者注
注12：截至2023年11月下旬，ChatCompletion可选参数已更新，其中functions和function_call已被弃用；新增了tools、response_format、seed等参数。请以最新的OpenAI文档为准。——译者注
注13：截至2023年11月下旬，ChatCompletion输出参数已更新，新增了system_fingerprint等参数。请以最新的OpenAI文档为准。——译者注
注14：截至2023年11月下旬，functions和function_call已被弃用，取而代之的是tools和tool_choice。关于函数对象的详细参数，请以最新的OpenAI文档为准。——译者注 
